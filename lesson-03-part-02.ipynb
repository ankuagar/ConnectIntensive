{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Intensive - Machine Learning Nanodegree\n",
    "# Lesson 03: Building and evaluating models with `sklearn`\n",
    "# Part 02: Building Predictive Models\n",
    "\n",
    "## Objectives\n",
    "  - Experiment with building predictive models using the [ `sklearn` library](http://scikit-learn.org/stable/).\n",
    "   - Learn about evaluation metrics for classification [ `sklearn.metrics` ]\n",
    "   - Confusion matrix, precision and recall \n",
    "   - Save cleaned datasets (so that we don't lose our hard preprocessing work!)\n",
    "  \n",
    "## Prerequisites\n",
    "  - You should have the following python packages installed:\n",
    "    - [matplotlib](http://matplotlib.org/index.html) (not a pre-reqisite for this part)\n",
    "    - [numpy](http://www.scipy.org/scipylib/download.html)\n",
    "    - [pandas](http://pandas.pydata.org/getpandas.html)\n",
    "    - [sklearn](http://scikit-learn.org/stable/install.html)\n",
    "  - If you're rusty on exploratory data analysis using `pandas`, you may want to check out lessons 01 and 02 in the [ConnectIntensive repo](https://github.com/nickypie/ConnectIntensive)\n",
    "\n",
    "\n",
    "## Acknowledgements\n",
    "  - This lesson is adapted from an introductory tutorial on pandas and scikit-learn from the Kaggle Titanic Compettion Website\n",
    "  (https://www.kaggle.com/c/titanic/forums/t/10125/pandas-and-scikit-learn-introduction-via-kaggle-titanic-competition).\n",
    "  - It also draws on material from part 1 of this lesson \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "As usual, we start by importing some useful libraries and modules. Don't worry if you get a warning message when importing `matplotlib` -- it just needs to build the font cache, and the warning is just to alert you that this may take a while the first time the cell is run.\n",
    "\n",
    "**Run** the cell below to import useful libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use('ggplot')\n",
    "    print(\"Successfully imported matplotlib.pyplot! (Version {})\".format(matplotlib.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import matplotlib.pyplot!\")\n",
    "    \n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Successfully imported numpy! (Version {})\".format(np.version.version))\n",
    "except ImportError:\n",
    "    print(\"Could not import numpy!\")\n",
    "    \n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Successfully imported pandas! (Version {})\".format(pd.__version__))\n",
    "    pd.options.display.max_rows = 10\n",
    "except ImportError:\n",
    "    print(\"Could not import pandas!\")\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    print(\"Successfully imported display from IPython.display!\")\n",
    "except ImportError:\n",
    "    print(\"Could not import display from IPython.display\")\n",
    "    \n",
    "try:\n",
    "    import sklearn\n",
    "    print(\"Successfully imported sklearn! (Version {})\".format(sklearn.__version__))\n",
    "    skversion = int(sklearn.__version__[2:4])\n",
    "except ImportError:\n",
    "    print(\"Could not import sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the pre-processed data\n",
    "We need to reload the data we saved in the previous session\n",
    "\n",
    "**Run** the cell below (**click** on the cell to highlight it, then press **shift + enter** or **shift + return** to run it) to read the training and testing data into `pandas` `DataFrame` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"lesson-03-data/titanic_train_cleaned.csv\")\n",
    "print(\"Pre-processed Titanic data sets loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Just to be sure we grabbed the right data, lets print a summary\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Looks like we inadvertently added a index column - lets take it out\n",
    "if u'Unnamed: 0' in train_df.columns:\n",
    "    print('Dropping \"unnamed\" column from train_df')\n",
    "    train_df = train_df.drop(u'Unnamed: 0', axis=1)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making some basic predictions\n",
    "\n",
    "Recall that the key feature we will attempt to predict is the `'Survived'` feature, which is equal to 0 or 1 for a passenger who died or survived, respectively, from the Titanic sinking. \n",
    "\n",
    "We'll try several sets of predictions and calculate some metrics to evaluate our 'model'\n",
    "\n",
    "A commonly used metric for classification is accuracy_score, which is simply the proportion of correct predictions. If a model predicts m classes of n possible correctly, then the accuracy score will be m / n.\n",
    "\n",
    "The accuracy_score simply ignores wrong predictions. In some situations, we may care about making wrong predictions; the F1 score is a measure that combines both correct and incorrect predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "no_survivors = np.array([0]*891)\n",
    "\n",
    "# sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)[source]\n",
    "print(\"Accuracy score: {:.2f}\".format(accuracy_score(train_df['Survived'], no_survivors)))\n",
    "\n",
    "print(\"Number perished: {}\".format(sum(train_df['Survived'] == 0)))\n",
    "print(\"Manual accuracy: {:.2f}\".format(549.0/891))\n",
    "\n",
    "# f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)[source]\n",
    "print(\"F1 score (survivors): {:.2f}\".format(f1_score(train_df['Survived'], no_survivors)))\n",
    "print(\"F1 score (non-survivors): {:.2f}\".format(f1_score(train_df['Survived'], no_survivors, pos_label=0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another \"model\" -- we predict everyone survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_survivors = np.array([1]*891)\n",
    "print(\"Accuracy score: {:.2f}\".format(accuracy_score(train_df['Survived'], all_survivors)))\n",
    "\n",
    "print(\"Number survived: {}\".format(sum(train_df['Survived'] == 1)))\n",
    "\n",
    "print(\"Manual accuracy: {:.2f}\".format(sum(train_df['Survived'] == 1)/891.0))\n",
    "print(\"F1 score (survivors): {:.2f}\".format(f1_score(train_df['Survived'], all_survivors)))\n",
    "print(\"F1 score (non-survivors): {:.2f}\".format(f1_score(train_df['Survived'], all_survivors, pos_label=0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Why are the two F1 scores different? Is one of them _correct_? If so, which one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Construct a model that predicts all females survived. What is the accuracy_score and F1 score for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Try some other model, e.g., all females and all males travelling first class survived. Calculate the accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix for binary classes is often used to provide a compact summary of correct and incorrect predictions. The ground truth is listed down the side and the predicted values are listed along the top. The actual values in each cell of the corresponding grid is the count of cases for which both the ground truth and the predicted value hold.\n",
    "\n",
    " Total Pop | Predicted cond is negative | Predicted cond is positive \n",
    " -------- | -------- | -------- \n",
    " Ground cond is False |  True Negative (TN) | False Positive (FP) \n",
    " Ground cond is True | False Negative (FN) | True Positive (TP)\n",
    " \n",
    " Some commonly used terms:\n",
    " - Precision = True Positive / All Positive = TP / ( TP + FP )\n",
    " - Recall = True Positive / All True = TP / (TP + FN)\n",
    " - Accuracy = True Positive + True Negative)/Total Population = (TP+TN)/(TP+TN+FP+FN)\n",
    " \n",
    " There are many other terms and ratios described in the lecture videos \n",
    " (also this Wiki article https://en.wikipedia.org/wiki/Precision_and_recall )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demo code for calculating a confusion mtrix for the case where everyone survived\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)[source]\n",
    "\n",
    "cc= confusion_matrix(train_df['Survived'], all_survivors)\n",
    "print \"Confusion matrix\", cc\n",
    "precision = cc[1,1]*1.0/sum(cc[:,1])\n",
    "recall = cc[1,1]*1.0/sum(cc[1,:])\n",
    "print(\"Precision = {:.2f}\".format(precision))\n",
    "print(\"Recall = {:.2f}\".format(recall))\n",
    "print(\"Manual F1-score = {:.2f}\".format(2.0*precision*recall/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "What is the precision and recall for your answer to question 2? The F1 score is defined as $ 2*Precision*Recall/(Precision + Recall) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Some other commonly used measures that can be calculated from the confusion matrix are:\n",
    "- Specificity = TN / (TP+TN)\n",
    "- Positive Likelihood Ratio (LR+) = (TP/(TP+FN)) / (FP/(FP+TN)) \n",
    "- Negative Likelihood Ratio (LR-) = ( FN/(FN+TP) ) / (TN/(TN+FP))\n",
    "- Odds Ratio = LR+/LR-\n",
    "\n",
    "Calculate the Specificity and LR+ for the \"all survived\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "There is a relatively simple recipe you can use for most of the modelling algorithms in sklearn\n",
    "\n",
    "### 1. Select the set of features in your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = train_df.columns.tolist()\n",
    "cols = [cols[1]] + cols[0:1] + cols[2:]\n",
    "df = train_df[['Survived','Pclass','Sex','Age','SibSp','Parch']]\n",
    "train_data = df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select the appropriate classifier or regressor you want to use, then import it from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create an instance of the classifier or regressor, with appropriate model parameters\n",
    "\n",
    "A learning algorithm adjusts the weights and other parameters during the fitting process. However, many algorithms also define and use some additional parameters that are in some sense part of the definition of the model we are trying to fit. These are sometimes called hyper-parameters. These need to be specified when the instance of the classifier or regressor is created. Most learners have defaults built in but it is important to know what those defaults are.\n",
    "\n",
    "We will learn about the subtleties of many different algorithms in the coming weeks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit the model to the training data\n",
    "This part generally takes two arguments, the training data and the labels (if a classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = model.fit(train_data[0:,1:],train_data[0:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predict the labels of the training data (or test data)\n",
    "\n",
    "Normally we would split the available data into a test and a training set and predict the labels of the test data set. We do have a test set, so can we use that here as we normally would? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=model.predict(train_data[0:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate the performance of the model (i.e., score the predicted results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(p, train_data[0:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Pick your own set of features and classifier and calculate the accuracy score of your predictions. Note that in this exercise we are doing this on the training set only and we can potentially get really good accuracy scores for the training set (why?).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  That's all for today !!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 08: Markov Decision Processes and Q Learning\n",
    "\n",
    "## Part 01: Markov Decision Processes\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook outlines the general concepts of Markov Decision Processes (MDPs) as discussed by Charles and Michael in the video lectures. Much of the content is scraped from the trnascripts of those videos, so you may hear their voices coming through here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Making  Reinforcement Learning\n",
    "\n",
    "Differences are between the three types of learning -  supervised, unsupervised and reinforcement. \n",
    " \n",
    "- ** Supervised learning ** takes the form of _function approximation_ where you're given a bunch of _x_, _y_ pairs (features and labels), and your goal is to find a function **f** that will map some new _x_ to a proper _y_. If the learner is a good one, then the predicted _y_ will be the same as or close to (in some sense) the true _y_. These _x_'s and _y_'s are often vectors.\n",
    "\n",
    "- ** Unsupervised learning ** takes a bunch of _x_'s and your goal is to find some \"function\" **f** that gives you a compact description (this is the equivalent of _y_ now) of the set of _x_'s that you've seen. So we call this _clustering_, or _description_ as opposed to function approximation.\n",
    "\n",
    "- ** Reinforcement learning (RL) **  takes a bunch of data pairs (_x_'s and _z_'s) and learns some function **f** that can generate the _y_'s. In this case, though, the _x_, _y_ and _z_ elements are very different from the the _x_ and _y_ in the other two types of learning. So the first step in RL is to understand what the _x_, _y_ and _z_'s are.\n",
    "\n",
    "We are going to motivate the definition of these _x_, _y_ and _z_'s with a simple game. The game is adapated from the video lecture (which in turn adapted it from one of the classic texts on AI -- **Artificial Inelligence - A Modern Approach**, _Stuart Russell and Peter Norvig_). \n",
    "\n",
    "### The \"World\" of Charles and Michael - Quiz\n",
    "\n",
    "In the video lectures, they describe a simple $ 3x4 $ grid-like world as the premise of a game. There are four kinds of cells in this world:\n",
    "    1. cells that you can move to without anything spectacular happening\n",
    "    2. cells that are \"walled\" and you cannot get to them\n",
    "    3. cells that are spectacularly bad (you LOSE) and the game ends if you land in one of them\n",
    "    4. cells that are pectacularly good (you WIN) and the game ends if you land in one of them. \n",
    "    \n",
    "The player starts of in one one of the cells (type 1) selectedly randomly -- the _start_ state. The play proceeds by the player selecting one of a set of actions (_up_, _down_, _left_, or _right_) that  moves the player to a different cell. The object of the game is for the player to get to a cell of type 3 so the player can win the game. Also, a player can never all off the grid; if they attempt to do so, they will end up not moving.\n",
    "\n",
    "Let's switch to using \"you\" instead of player (much easier for me to use!). In the grid displayed below we represent a $ 4x4 $ world. **X** represents \"walled\" cells; if you take an action attempting to move into a walled cell, you end up not moving. At every step, the game's controller responds to your actions deterministically, i.e., if you pick _L_ (to go _left_), the controller moves you to a cell to the immediate left of their current position provided that were an available cell (i.e., not a waled cell or a boundary). \n",
    "\n",
    "\n",
    "| A | B | C | D | \n",
    " ----- | ------ | ------------ | --------- | ------------- | ----- \n",
    "a |   <code>       </code> |  **X**  | <code>       </code>   |  _WIN_  \n",
    "b |    |  <code>       </code> |    |  _LOSE_ \n",
    "c |    | **X**  |  <code>       </code>  |   <code>       </code> \n",
    "d | _start_  |   |    |   \n",
    "  \n",
    "**Question 1.** In this world, given a _start_ position of d,A (lower left corner), what are the two optimal (shortest) paths to winning the game? How many moves did it take? What is the probability that the player wins the game?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Answer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the world described above, I expect you were able to find the path without too much trouble. Think about how you solved this problem. What were the steps you took? What were the decisions you made along the way? Please discuss with your partner and write them down. It may seem a bit silly, but there is a small point to it. What algorithm might you use to solve this for a grid of any dimension where the world followed the same rules (you don't have to write the algorithm, just think of a way you might be able to do it)? \n",
    "\n",
    "** Reflection 1** How I solved the path problem\n",
    "\n",
    "** Answer: **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > _Answer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next version of the game, we consider the case where the rules of movement have changed slightly. Now, the game's controller _doesn't_ always move you in the direction you wanted to move. Instead, 80% of the time, the controller moves you in the direction, but 20% of the time, it moves you in a direction perpendicular to the one you wanted to take. For example, let's say from the _start_ position (d,A), you wanted to go up. 80% of the time you would go up to (c,A), 10% of the time you would move to (d,B) and 10% of the time you would remain in (d, A) as the controller would try to move you to the right, but there is a boundary there so you end up not moving at all.\n",
    "\n",
    "** Question 2.** Pick one of the paths from your answer to Q1. What is the probability that you will win the game if you made the exact same sequence of moves? Remember the minor cases.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code cell -- you can write out all the steps and probabilities and use python as a simple calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some food for thought. How easily could you calculate the probability of winning the game? Are you guaranteed to win it every time? What kind of algorithm might you use to find the optimal path to the WIN state? Discuss these with your partner. Write down some of your thoughts (I am not looking for the correct answer).\n",
    "\n",
    "** Reflection ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making decisions in a stochastic world \n",
    "\n",
    "The exercise in the videos (and above) illusrate how it is possible to come up with a static plan when the world is deterministic, but the problem becomes a bit more challenging when there is uncertainty (or stochasticity). We have two options:\n",
    " - plan out what we would do in a deterministic world and try to execute the steps of the plan, then every once in a while, see if we've drifted away from where we thought we should be, re-evaluate, draw up a new plan (still assuming a deterministic world)\n",
    " - come up with some way to incorporate all of these uncertainties and probabilities so that we never really have to rethink what to do in case something goes wrong and live with the consequences. Some fraction of the time we would end up not achieving our goal of reaching the WIN position, but that we would have the best chance of winning.\n",
    " \n",
    "The second method is called single agent reinforcement learning, or a Markov Decision Process (MDP). \n",
    "\n",
    "Let's look back at our reflections on question 1. You may have come up with a different way, but generally, we look at the grid and say there are specific locations and I need to move from one location to the next. We are going to abstract these words and call the location (a grid cell) a **_state_** and the moves we made an **_action_**. These two terms are essential concepts in the world of RL or MDPs. \n",
    "\n",
    "Continuing on with adding more words to our thought processes, in solving the grid-world problem, we were in a state (location), took an action and ended up in another state. The grid was laid out for us so we could see the connections between the states and knew that we had to go _up_ from (d, A) to get to (c, A).  Well this map can also be abstracted and is another important concept -- the **_model_** or the **_transition model_**. It is a function of three variables, a state $ s $, an action $a$ and another state (where we end up) $s'$. In very broad terms, the colection of states define our world or universe, the actions are the things we can do to explore the world, and the transition model encapsulates the rules of the game.\n",
    "\n",
    "**Question 3:** Using notation like (d,A) to represent a state, one of the four letters U, D, L, R to denote the action, write down all possible entries in the transition model for the deterministic world (from question 1) starting in states (b,C) and (c, A). Each state should have four entries. If the move isn't permitted, then the \"final\" state is the same as the initial state.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " >  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that in the simplistic model, we left out explicitly mentioning a few key points. The first one in the _Markovian property_ which essentially states that its only the present state that matters. The history of how we got there doesn't matter. The second one is the non-stationarity property\n",
    "\n",
    "The three abstract concepts are all we need to 'live' in the world, but if we want to have a purpose, we need to add the notion of a **reward**. A **reward** is simply a scalar value that you get for being in a state. It encompasses our domain knowledge; the reward you get from the state tells you the usefulness of entering into that state. Why is this important? Without a reward, there is no reason to be in any state vs. the next and also no real need to make decisions. \n",
    "\n",
    "These four things, by themselves, along with this Markov property and non-stationarity. , defines what's called the Markov \n",
    " Decision Process. Or an MDP. Got it? \n",
    "\n",
    "## 10 - Markov Decision Processes - 4.srt\n",
    "\n",
    " Speaking of solutions, this is the last little bit of thing that you need to know. And that is. This defines a \n",
    " problem. But, what we also want to have, whenever we have a problem. Is a solution. So, the solution to the Markov \n",
    " Decision Process, is something called a policy. And, what a policy does. Is, it's a function, that takes in a state. And \n",
    " returns an action, in other words, for any given state that you're in, it tells you the action that you should take. \n",
    " \n",
    " Like as a hint? \n",
    " No, it just tells you, this is the at. Well, I mean, I suppose you don't have to do \n",
    " it, but the way we think about Markov Decision Processes, is that this is the action that will be taken. \n",
    " \n",
    " I see, so it's more of an order. \n",
    " Yes, it's a command. Okay. \n",
    " So that's all a policy is. A policy is \n",
    " solution to a Markov Decision Process. And there is a special policy, which I'm writing here as policy star, or \n",
    " the optimal policy, and that is the policy that maximizes your long-term expected reward. So if all the policies you \n",
    " could take, of all the decisions you might take, this is the policy that optimizes the amount of reward that \n",
    " you're going to receive or expect to receive over your lifetime. \n",
    " So, like, at the end? \n",
    " Well, at yeah, at the end, or at \n",
    " any given point in time, how much reward you're receiving. >From the Markov Decision Process point of view, there doesn't \n",
    " have to be an end. Okay. Though in this example, you don't get anything, and then at the end, you get paid off. \n",
    " \n",
    " Right, or unpaid off. \n",
    " Right. \n",
    " If you fall into the red square. So actually, your question points out something very important here. I \n",
    " mentioned earlier when I talked about the three kinds of learning that there, supervised learning and reinforced learning were sort \n",
    " of. Similar, except that instead of getting Ys and Xs we were given Ys and, Xs and Zs. And this is exactly what's happening here. Here what we would \n",
    " like to have if we wanted to learn a policy is a bunch of sa pairs as training examples. Well here's the state and the action you should've took, taken, here's \n",
    " another state and the action you should've taken, so on and so forth. And then we would learn a function, the policy, \n",
    " that maps states to actions. But what we actually see in the reinforcement learning world, in the Markov Decision Process world, \n",
    " is we see states, actions, and then the rewards that we received. And so in fact, this problem of seeing a sequence \n",
    " of states, actions, and rewards. It's very different from the problem of being told. This is the correct action to take to maximize a function. \n",
    " Or find a function that maps from state to action. Instead, we say well, if you're in this state, and you take this action, this is \n",
    " the reward that you would see. And then from that, we need to find the optimal action. \n",
    " So Pi star is being the F from that previous slide? \n",
    " \n",
    " Right. \n",
    " And R is being Z? \n",
    " Yes. And y is being a. \n",
    " And s is being x or x is being s \n",
    " Got you. \n",
    " Right. \n",
    " \n",
    " So but, I'm, okay I'm a little confused about this notion of a policy. So we have the, the, the thing we tried to do to \n",
    " get the goals was up, up, right, right, right. Yes. \n",
    " I don't see how to capture that as a policy. \n",
    " It's actually fairly straightforward. What a policy would say is: \n",
    " What state are you in? Tell me what action you should take. So, the policy, basically is this: When you're in \n",
    " the state, start, the start state, the action you should take is up. And it would have a mapping. For every state \n",
    " that you might see, whether it's this state, this state, this state, this state, this state, this state, this state, or \n",
    " even these two states, and it will tell you what action you should take. And that's what a policy is. A policy, \n",
    " very simply, is nothing more than a function that tells you what action to take at every, in any state you happen to come across. \n",
    " \n",
    " Okay, but the, but the. The question that you asked before was about up, up, right, right, right. \n",
    " Mm hm. \n",
    " \n",
    " And, it seems like, because of the stochastic transitions. You might not be in the same state. Like, you don't know what \n",
    " state you're in, when you take those actions. \n",
    " No, so, one of the things for what we're talking about \n",
    " here, for the Markov Decision Process. Is, there're states, there're actions, there're rewards. You always know what state you're in, and you know what reward you \n",
    "receive. \n",
    " So does that mean you can't do up, up right right right? \n",
    " Well, the way it would work in a Markov Decision Process, \n",
    " so what you're describing is is what's often called a plan. You know, it's, tell me what sequence of actions I should take from \n",
    " here. What Markov Decision Process does and what a pr, a policy does is it doesn't tell you what sequence of actions to take fr om\n",
    "a particular state. It tells you what action to take in a particular state. You will then end up in another state because of \n",
    " the transition model, the transition function. And then when you're in that state you ask the policy what actions should I take now? Okay. \n",
    " \n",
    " Right, so this is actually a key point. Although we talked about it in the language of planning, which is very common for the people who di, for example \n",
    " take any ag course, the thing about this in terms of planning, what are the things that I can do to accomplish my \n",
    " goals? The Markov Decision Process way of thinking about it, the reinforcement way of thinking about it, or the typical reinforcement lea rning\n",
    "way of thinking about it, really doesn't talk about plans directly. But instead, talks about policies. Which from which you can infer a pl an,\n",
    "but this has the advantage that it tells you what to do everywhere. And it's robust to the underlying stochastic of the word .\n",
    "World. \n",
    " So, is it clear that's all you need to be able to behave well. \n",
    " Well, it's certainly the case, that if you have a pol icy\n",
    "and that policy is optimal, it does tell you what to do, no matter what situation you're in. \n",
    " 'Kay. \n",
    " And so, if you have that, then that's definitely \n",
    " all you need to behave well. But I mean could it be that you wanted to do something like up, up, right, right, right which you cant write down as a po licy?\n",
    " \n",
    " And why cant you write that down as a policy? \n",
    " Because the policies are only telling you what act ion\n",
    "to do as a function of the state not sort of like how far along you are in the sequence. \n",
    " Right unless, of course, you fold that into your  state\n",
    "some how. But thats exactly right, the way to think about this is. The idea of coming up with a concrete plan of what to do for the next 20 time \n",
    "steps is different from the problem of whatever step I happen to be in, whatever state I happen to be in, what's the next best thing \n",
    " I can do? And just always asking that question. \n",
    " Hm. \n",
    " If you always ask that question, that will induce a sequence, but that sequence \n",
    " is actually dependent upon the set of states that you see. Whereas in the other case where we wrote down a particular policy, you'll \n",
    " notice that was only dependent upon the state you started in and it had to ignore the states \n",
    "that you saw along the way. \n",
    " And the only way to fix that would be to say, well, after I've taken an action, let \n",
    "me look at the state I'm in and see if I should do something different with it. But if you're going to do that, then why are you tr ying\n",
    "to compute the complete set of states? Or I'm sorry, the complete set of actions that you might take. \n",
    " Okay. \n",
    " \n",
    " Okay, so there you go. Now, a lot of what we're going to be talking about next Michael, is, given that we have MDP, we have this Marko v\n",
    "Decision Process defined like this. How do we go from this problem definition to finding a good poli cy,\n",
    "and in particular, finding the optimal policy? That makes sense. \n",
    " Good. And there you go."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 07: Unsupervised Learning - continued\n",
    "\n",
    "### Part 01: Comparing Different Clustering Algorithms\n",
    "\n",
    "\n",
    "Last week, we used k-means clustering to the Enron financial dataset available in the [ud120-projects repo](https://www.github.com/udacity/ud120-projects). In today's lesson, we will work through a few other clustering algorithms and consider scenarios where one method is preferred over the others. The examples and code are drawn from the [sklearn documentation](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)\n",
    "\n",
    "We will look at the results of a few different clustering algorithms on \"interesting\" (or pathological) datasets with two independent variables. These examples will give us some intuition about which clustering algorithm might perform better in a given scenario. \n",
    "\n",
    "With this intuition, we will apply these techniques to the Enron dataset and compare our findings with the results of K-means clustering exercise we completed last week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries \n",
    "As usual, we start by loading the packages we are going to use. These should be already by installed in your environment. There are no new packages that need to be installed for this session\n",
    "\n",
    " - numpy\n",
    " - pandas\n",
    " - sklearn (this version of the notebook uses v0.18). \n",
    " - matplotlib\n",
    " \n",
    "\n",
    " Run the cell below to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python \n",
    "\n",
    "import pickle\n",
    "import os, sys\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Successfully imported numpy! (Version {})\".format(np.version.version))\n",
    "except ImportError:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"Successfully imported matplotlib! (Version {})\".format(matplotlib.__version__))\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Successfully imported pandas! (Version {})\".format(pd.__version__))\n",
    "    pd.options.display.max_rows = 10\n",
    "except ImportError:\n",
    "    print(\"Could not import pandas!\")\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    from IPython.display import Image\n",
    "    print(\"Successfully imported display from IPython.display and Image!\")\n",
    "except ImportError:\n",
    "    print(\"Could not import display from IPython.display\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from ipython notebook from http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n",
    "import time\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.mixture import GMM\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)\n",
    "\n",
    "clustering_names = [\n",
    "    'MiniBatchKMeans', 'AgglomerativeClustering',  'DBSCAN', 'GMM' ]\n",
    "\n",
    "\n",
    "datasets = [noisy_circles, noisy_moons, blobs, no_structure];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of clustering is to find entities (records, rows) that are most similar to each other and group them together. All clustering algorithms rely on some measure of \"distance\" to determine similarity. For example, the k-Means algorthm we used last time uses Euclidean distance. Most clusterers in `sklearn` have a parameter for defining this distance. \n",
    "\n",
    "Since the algorithm is sensitive to distance by design, it is very important that all features (variables) are normalized. In this section, we are using `StandardScaler` from `sklearn.preprocessing`. _Notice the difference between the results of `MinMaxSaler` and `StandardScaler`. Is one of them better to use in this context?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll just take one dataset and look at its statistics before and after scaling\n",
    "\n",
    "X, y = blobs\n",
    "XS = StandardScaler().fit_transform(X)\n",
    "print \"{:<15}  Min={:.3f}, Median={:.3f}, Average={:.3f}, Max={:.3f}\".format('Raw values',np.min(X), np.median(X), np.mean(X), np.max(X))\n",
    "print \"{:<15}  Min={:.3f}, Median={:.3f}, Average={:.3f}, Max={:.3f}\".format('StandardScaler', np.min(XS), np.median(XS), np.mean(XS), np.max(XS))\n",
    "XS = MinMaxScaler().fit_transform(X)\n",
    "print \"{:<15}  Min={:.3f}, Median={:.3f}, Average={:.3f}, Max={:.3f}\".format('MinMaxScaler', np.min(XS), np.median(XS), np.mean(XS), np.max(XS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have engineered four datasets:\n",
    "- two concentric circles (circles)\n",
    "- two partial moons (moons)\n",
    "- three centers, two relatively close and the third well-separated (blobs)\n",
    "- unstructured (no structure)\n",
    "Each dataset has gaussian noise added to it -- hence the prefix \"noisy\" for the first three datasets.\n",
    "\n",
    "We will find the 'best' cluster assignments assuming 2 clusters, using four different algorthms:\n",
    "- k-means \n",
    "- hierarchical (Agglomerative) clustering\n",
    "- DBSCAN (an algorithm not discussed in the lectures, but falls in the same category as HACs)\n",
    "- Gaussian Mixture Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(len(clustering_names) * 2 + 3, 9.5));\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01);\n",
    "\n",
    "plot_num = 1;\n",
    "\n",
    "for i_dataset, dataset in enumerate(datasets):\n",
    "    X, y = dataset\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # connectivity matrix for Agglomerative clustering\n",
    "    connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "    \n",
    "    # create clustering estimators\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=2)\n",
    "    dbscan = cluster.DBSCAN(eps=.2)\n",
    "    gmm = GMM(n_components=2, tol=.2)\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\", n_clusters=2,\n",
    "        connectivity=connectivity)\n",
    "\n",
    "    clustering_algorithms = [\n",
    "        two_means,  average_linkage,  dbscan, gmm ]\n",
    "\n",
    "    for name, algorithm in zip(clustering_names, clustering_algorithms):\n",
    "        # predict cluster memberships\n",
    "        t0 = time.time()\n",
    "        algorithm.fit(X)\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        # plot\n",
    "        plt.subplot(4, len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n",
    "\n",
    "        if hasattr(algorithm, 'cluster_centers_'):\n",
    "            centers = algorithm.cluster_centers_\n",
    "            center_colors = colors[:len(centers)]\n",
    "            plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
    "        plt.xlim(-2, 2)\n",
    "        plt.ylim(-2, 2)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Please look over the results and write down why each algorithm came up with the answers it came up with (some of them are obviously wrong!) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_** Answer **_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering and dendrograms\n",
    "In the video lectures on Single Link Clustering, Charles and Michael described how HACs are constructed. You also need to choose how to measure the distance between two points (or elements), a point and a cluster, as well as two clusters. _In the code block above, which two parameters are being used for calculating 'distance'?_\n",
    "\n",
    "It is also interesting to visualize how these clusters are built up. The cell below plots a \"dendrogram\" (an entity with many levels of branching). A key point to remember is that every horizontal line identifying two clusters that get merged has its own height (difficult to see because of the scaling). Once the complete tree structure as been created, you can draw a horizontal line at an appropriate height to get an arbitrary number of clusters (of course can't have more than the number of leaves you started with!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/#Plotting-a-Dendrogram\n",
    "#\n",
    "# generate the linkage matrix\n",
    "Z = linkage(X, 'average')\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Height')\n",
    "\n",
    "\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=12,  # show only the last p merged clusters\n",
    "    show_leaf_counts=False,  # otherwise numbers in brackets are counts\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models and Expectation Maximization (EM)\n",
    "\n",
    "The mixture model is inherently different from the other clustering algorithms we used in that  it determines a probability that any given case belongs to one of the _n_ clusters that it was supposed to find rather than a hard (0 or 1) assignment to a cluster. This can be very helpful when the clusters are overlapping.\n",
    "\n",
    "#### TODO (Add some more detail on how this works - )\n",
    "\n",
    "In order to explore mixture models in a better context, we will apply it to the classis iris data set from the [UCI ML repository](http://mlearn.ics.uci.edu/MLRepository.html). We don't have to go out and download it as it is already bundled into `sklearn`. \n",
    "\n",
    "Here is a description of the iris data set from [Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n",
    ">The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".\n",
    "\n",
    ">The data set consists of 50 samples from each of three species of Iris (_Iris setosa_, _Iris virginica_ and _Iris versicolor_). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n",
    "\n",
    "** _As part of this exercise, we will also try an interactive 3D visualization. Unfortunately, the interactive part won't run in the notebook, so we will have to suspend the inline plot option temporarily_ **\n",
    "\n",
    "Run the next cell to import the matplotlib toolkit for 3d plots and load the definition of our interactive plot function. \n",
    "The 3D plot should pop up only after you run the cell after the next one (one which pulls in the iris data and invokes Draw3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Modified for Udacity ConnectIntensive training\n",
    "# Lutfur Khundkar\n",
    "\n",
    "\n",
    "%matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#import matplotlib.animation as animation\n",
    "\n",
    "def Draw3D(X,Y, labels=['X', 'Y', 'Z']):\n",
    "    # To getter a better understanding of interaction of the dimensions\n",
    "    # plot the first three PCA dimensions\n",
    "    fig = plt.figure(1, figsize=(8, 6))\n",
    "    ax = Axes3D(fig, elev=-150, azim=110)\n",
    "    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=Y, s=50.0,\n",
    "               cmap='viridis')\n",
    "    ax.set_title(\"Ineractive 3D plot\")\n",
    "    ax.set_xlabel(labels[0])\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.set_ylabel(labels[1])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.set_zlabel(labels[2])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "    angle = 60.0\n",
    "    ax.view_init(30, angle)\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import datasets\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
    "Draw3D(X_reduced,Y,['PCA '+str(i+1) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the next cell, use GMM to create 3 clusters from the iris dataset. Since we have the labels in Y, find the `silhouette_score` \n",
    "\n",
    "####  Answer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of the basic exercise for this notebook. \n",
    "\n",
    "### Further Exploration\n",
    "\n",
    "If you have time and want to pursue this a bit more, you can explore PCA and GMM on the enron data set we worked with last week. I have included a shortened version of that notebook in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TODO - adjust the value of the PATH_TO_MINI variable so it points to the top level folder of the Udacity projects\n",
    "##     For example, here is the structure on my machine:\n",
    "###         parent dir (projects)\n",
    "###               -  ZKConnect (folder for ConnectIntensive notebooks\n",
    "###                     - lesson-06-part-01,ipynb (this file)\n",
    "###               -  projects  (root folder for all Udacity projects - github ud120) \n",
    "###                     - final_project\n",
    "###                     - k_means\n",
    "###                     - naive_bayes\n",
    "###                     - tools\n",
    "###\n",
    "### So, I would set PATH_TO_MINI = '../projects'\n",
    "###\n",
    "### Once this is correctly set using either a relative path or an absolute path, the rest of the code should work correctly\n",
    "\n",
    "PATH_TO_MINI = \"../projects\" # This may be different for each user\n",
    "\n",
    "### Once you have the path set, the output from this cell should say so\n",
    "\n",
    "try:\n",
    "    path_ok = os.path.isfile(os.path.join(PATH_TO_MINI,\"final_project\",\"final_project_dataset.pkl\"))\n",
    "    if not path_ok:\n",
    "        raise Exception(\"Path is not set correctly\")\n",
    "    print \"PATH_TO_MINI appears correct (can open project data file)\" \n",
    "except Exception as e:\n",
    "    print e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(PATH_TO_MINI)\n",
    "sys.path.append(os.path.join(PATH_TO_MINI,\"tools\"))\n",
    "sys.path.append(os.path.join(PATH_TO_MINI,\"k_means\"))\n",
    "\n",
    "## Adapted from ud120-projects/k_means/k_means_cluster.py\n",
    "from feature_format import featureFormat, targetFeatureSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration and Cleaning\n",
    "\n",
    "We'll skip the basic data exploration here today as we are familiar with the dataset. Howeve, we do need to remove the \"TOTAL\" key and impute the missing values (_NaN_) with 0. We will use `pandas` data frame to do the cleaning necessary. You can use the Draw3D function to getter a better visualization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### load in the dict of dicts containing all the data on each person in the dataset\n",
    "data_dict = pickle.load( open(os.path.join(PATH_TO_MINI,\"final_project\",\"final_project_dataset.pkl\"), \"r\"))\n",
    "print \"Loaded dataset - has {} keys (or rows)\".format( len(data_dict.keys()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove \"TOTAL\" and replace missing values with 0.\n",
    "KEYS_TO_REMOVE = [\"TOTAL\"]\n",
    "\n",
    "for k in KEYS_TO_REMOVE:\n",
    "    if data_dict.pop(k, 0):\n",
    "        print \"Removed item {} from data set\".format(k)\n",
    "\n",
    "print \"Dataset with outliers removed has {} keys (or rows)\".format( len(data_dict.keys()) )\n",
    "\n",
    "# Create a DataFrame object from the Enron data dictionary\n",
    "enron_df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "# Take the transpose (.T) of the Enron DataFrame,\n",
    "enron_df = enron_df.T\n",
    "\n",
    "# Change all entries in the DataFrame with \"NaN\" to zeroes.\n",
    "print \"Replace all NaN's with 0\"\n",
    "enron_df[enron_df == \"NaN\"] = 0\n",
    "\n",
    "# Now remove rows that are all zeros -- this was being done in the feature_format.py tool, we'll do it with pandas\n",
    "orig_shape = enron_df.shape\n",
    "enron_df[(enron_df !=0).any(axis=1)]\n",
    "print \"{} rows with all zeros were dropped\".format('No' if orig_shape == enron_df.shape else str(orig.shape[0]-enron_df.shape[0]))\n",
    "\n",
    "\n",
    "# List of columns, i.e. features\n",
    "print \"List of column labels:\"\n",
    "colnum = 4\n",
    "colnames = [str(col) for col in enron_df.columns]\n",
    "column_count = len(colnames)\n",
    "blanks=[]*colnum\n",
    "for i in xrange(0, column_count, colnum):\n",
    "    fmt = \"{:<28} \"*colnum\n",
    "    try:\n",
    "        pp=colnames[i:min(i+colnum, column_count)]+blanks\n",
    "        print fmt.format(*pp)\n",
    "    except Exception as  e:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the features (i.e., columns or variables) available in this dataset? Which ones might be useful for identifying a poi? Select your top 5 choices and see if you there are combinations that seem to offer cleaner segments. You can use PCA and or GMM here to explore beyond the scope of the mini-project from last week.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = colors = np.where(enron_df.poi == 1, 'r', 'c')\n",
    "features_select = [\"salary\", 'exercised_stock_options', 'long_term_incentive']\n",
    "enron_data = np.array(enron_df[features_select])\n",
    "\n",
    "## May want to scale and tranform the data\n",
    "#enron_data_scaled = \n",
    "Draw3D(enron_data, colors, features_select)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

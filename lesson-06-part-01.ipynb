{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 06: Unsupervised Learning - Clustering and Principal Component Analysis\n",
    "\n",
    "### Part 01: Clustering with K-Means\n",
    "\n",
    "\n",
    "#### Clustering Mini Project \n",
    "\n",
    "In this project, we’ll apply k-means clustering to our Enron financial data available in the [ud120-projects repo](https://www.github.com/udacity/ud120-projects). It is best if you save a copy on your local machine (using git clone or download and extract the zip archive). You will need to specify the location of the folder where you placed the files from the repository. \n",
    "\n",
    "Our final goal, of course, is to identify persons of interest; since we have labeled data, this is not a question that particularly calls for an unsupervised approach like k-means clustering. \n",
    "\n",
    "Nonetheless, it is interesting to attempt a clustering analysis to see how well this kind of approach can distinguish between poi's and non-poi's. You’ll also get some hands-on practice with k-means in this project, and play around with feature scaling, which will give you a sneak preview of the next lesson’s material.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries \n",
    "As usual, we start by loading the packages we are going to use. These should be already by installed in your environment. There are no new packages that need to be installed for this session\n",
    "\n",
    " - numpy\n",
    " - pandas\n",
    " - sklearn\n",
    " - matplotlib\n",
    " \n",
    "In addition, we will be using data and some code from the ud120-projects repo, so those should be available on your local machine as well\n",
    " \n",
    " Run the cell below to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python \n",
    "\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Successfully imported numpy! (Version {})\".format(np.version.version))\n",
    "except ImportError:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"Successfully imported matplotlib! (Version {})\".format(matplotlib.__version__))\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Successfully imported pandas! (Version {})\".format(pd.__version__))\n",
    "    pd.options.display.max_rows = 10\n",
    "except ImportError:\n",
    "    print(\"Could not import pandas!\")\n",
    "\n",
    "import os, sys\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    from IPython.display import Image\n",
    "    print(\"Successfully imported display from IPython.display and Image!\")\n",
    "except ImportError:\n",
    "    print(\"Could not import display from IPython.display\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TODO - adjust the value of the PATH_TO_MINI variable so it points to the top level folder of the Udacity projects\n",
    "##     For example, here is the structure on my machine:\n",
    "###         parent dir (projects)\n",
    "###               -  ZKConnect (folder for ConnectIntensive notebooks\n",
    "###                     - lesson-06-part-01,ipynb (this file)\n",
    "###               -  projects  (root folder for all Udacity projects - github ud120) \n",
    "###                     - final_project\n",
    "###                     - k_means\n",
    "###                     - naive_bayes\n",
    "###                     - tools\n",
    "###\n",
    "### So, I would set PATH_TO_MINI = '../projects'\n",
    "###\n",
    "### Once this is correctly set using either a relative path or an absolute path, the rest of the code should work correctly\n",
    "\n",
    "PATH_TO_MINI = \"../projects\"\n",
    "\n",
    "### Once you have the path set, the output from this cell should say so\n",
    "\n",
    "try:\n",
    "    path_ok = os.path.isfile(os.path.join(PATH_TO_MINI,\"final_project\",\"final_project_dataset.pkl\"))\n",
    "    if not path_ok:\n",
    "        raise Exception(\"Path is not set correctly\")\n",
    "    print \"PATH_TO_MINI appears correct (can open project data file)\" \n",
    "except Exception as e:\n",
    "    print e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(PATH_TO_MINI)\n",
    "sys.path.append(os.path.join(PATH_TO_MINI,\"tools\"))\n",
    "sys.path.append(os.path.join(PATH_TO_MINI,\"k_means\"))\n",
    "\n",
    "## Adapted from ud120-projects/k_means/k_means_cluster.py\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "def Draw(pred, features, poi, mark_poi=False, name=\"image.png\", f1_name=\"feature 1\", f2_name=\"feature 2\"):\n",
    "    \"\"\" some plotting code designed to help you visualize your clusters \"\"\"\n",
    "\n",
    "    ### plot each cluster with a different color--add more colors for\n",
    "    ### drawing more than five clusters\n",
    "    colors = [\"b\", \"c\", \"k\", \"m\", \"g\"]\n",
    "    for ii, pp in enumerate(pred):\n",
    "        plt.scatter(features[ii][0], features[ii][1], color = colors[pred[ii]])\n",
    "\n",
    "    ### if you like, place red stars over points that are POIs (just for funsies)\n",
    "    if mark_poi:\n",
    "        for ii, pp in enumerate(pred):\n",
    "            if poi[ii]:\n",
    "                plt.scatter(features[ii][0], features[ii][1], color=\"r\", marker=\"*\")\n",
    "    plt.xlabel(f1_name)\n",
    "    plt.ylabel(f2_name)\n",
    "    #plt.savefig(name) # We will not save the figures as images -- can see them in the notebook\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### load in the dict of dicts containing all the data on each person in the dataset\n",
    "data_dict = pickle.load( open(os.path.join(PATH_TO_MINI,\"final_project\",\"final_project_dataset.pkl\"), \"r\"))\n",
    "print \"Loaded dataset - has {} keys (or rows)\".format( len(data_dict.keys()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration\n",
    "\n",
    "We want to get a sense of what is in the data set, whether there are any missing values that need to be cleaned up. \n",
    "\n",
    "##### Question 1 - How many different persons are there in this dataset? Are there any keys that should be excluded? (HINT: There is at least one). Use the $ pop $ method of a python $ dict $ object to remove any \"rows\" that need to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There's at least one row that doesn't belong in the dataset (an outlier?) --remove it! \n",
    "# You can print out the keys to see if there is one that is not a person\n",
    "# Should you be concerned about duplicates?\n",
    "\n",
    "### TOUR CODE CAN GO HERE\n",
    "\n",
    "#   Add the key of the \"row\" you want to remove as a string literal element in the KEYS_TO_REMOVE LIST\n",
    "KEYS_TO_REMOVE = [\"\"]\n",
    "\n",
    "for k in KEYS_TO_REMOVE:\n",
    "    if data_dict.pop(k, 0):\n",
    "        print \"Removed item {} from data set\".format(k)\n",
    "\n",
    "print \"Dataset with outliers removed has {} keys (or rows)\".format( len(data_dict.keys()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen before, `pandas` dataframes provide a convenient way of managing data. We will use the `from_dict` method of a DataFrame object to load our python data_dict. However, when the dictionary object loaded, the resulting DataFrame has the list of keys as its columns. We can \"flip\" this using the transpose (.T) so that each row contains the data for one person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame object from the Enron data dictionary\n",
    "enron_df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "# Take the transpose (.T) of the Enron DataFrame,\n",
    "enron_df = enron_df.T\n",
    "\n",
    "# Display the DataFrame after preprocessing is complete\n",
    "display(enron_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, several of the features have missing values and we will need to get rid of them using imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change all entries in the DataFrame with \"NaN\" to zeroes.\n",
    "enron_df[enron_df == \"NaN\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Question 2 - Why is it ok to replace the missing values with 0? Write your thoughts in the cell below and we will discuss as a group. Think about the other ways you can impute missing data, e.g., mean or median values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 - What are the features (i.e., columns or variables) available in this dataset? Which ones might be useful for identifying a poi? Please write down your top 5 choices along with your reasons for choosing them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can use the DataFrame we created with the sklearn routines, we will convert back to the python dictionary format so we can use the python code included with the projects for self-consistency. Also, we will save a separate copy of the data so we can switch between looking at the pre-processed data as well as the raw data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dict_new = enron_df.T.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next several cells are ones you can run repeatedly, varying the inputs to just play around.\n",
    "\n",
    "##### The online verison of the mini project has six quizzes. Some of the questions in this section are directed at addressing the quiz. You can use this notebook to complete the online version if you wish to do so.\n",
    "\n",
    "Select two features from the set you identified in Question 3 and  to use in the exploration below. Enter them as feature_1  and  feature_2  below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering with two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### the input features we want to use \n",
    "### can be any key in the person-level dictionary (salary, director_fees, etc.) \n",
    "\n",
    "### \n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "poi  = \"poi\"\n",
    "features_list = [poi, feature_1, feature_2]\n",
    "data = featureFormat(data_dict_new, features_list )\n",
    "poi, finance_features = targetFeatureSplit( data )\n",
    "\n",
    "\n",
    "### in the \"clustering with 3 features\" part of the mini-project,\n",
    "### you'll want to change this line to \n",
    "### for f1, f2, _ in finance_features:\n",
    "### (as it's currently written, the line below assumes 2 features)\n",
    "for f1, f2, in finance_features:\n",
    "    plt.scatter( f1, f2 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### cluster here; create predictions of the cluster labels\n",
    "### for the data and store them to a list called pred\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clf=KMeans(n_clusters=2, n_init=10, init=\"k-means++\")\n",
    "clf.fit(finance_features, poi)\n",
    "pred = clf.predict(finance_features) # The cluster assignements will be used to visualize them \n",
    "\n",
    "\n",
    "### We left out the \"name\" parameter as we don't want to save the images \n",
    "\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False,  f1_name=feature_1, f2_name=feature_2)\n",
    "    pass\n",
    "except NameError:\n",
    "    print \"no predictions object named pred found, no clusters to plot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### In the scatterplot that pops up, are the clusters what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three features clustering\n",
    "\n",
    "In this section we will add a third feature to features_list and rerun the clustering. The quiz specifies which feature to use  (\"total payments\"), but feel free to experiment with others. \n",
    "\n",
    "You can copy and modify the cells from the two features groups above to complete this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 - Compare the plot with the clusterings to the one you obtained with 2 input features. Do any points switch clusters? How many? This new clustering, using 3 features, couldn’t have been guessed by eye--it was the k-means algorithm that identified it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling \n",
    "\n",
    "As you can see, if you attempted to use one of the finance features as a variable, most of the \"points\" are close together but there are several that are fairly far away from the rest. Feature scaling is way to \"normalize\" the data so we don't end up emphasizing any one dimension too much.\n",
    "\n",
    "The next few questions (aligned with the online mini-project quizzes) lead you through some of the calculations that are done for feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the maximum and minimum values taken by the “exercised_stock_options” feature used in this example?\n",
    "\n",
    "(NB: if you look at finance_features, there are some \"NaN\" values that have been cleaned away and replaced with zeroes--so while those might look like the minima, it's a bit deceptive because they're more like points for which we don't have information, and just have to put in a number. So for this question, go back to `data_dict` and look for the maximum and minimum numbers that show up there, ignoring all the \"NaN\" entries.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the maximum and minimum values taken by “salary”?\n",
    "\n",
    "This feature also had NaNs in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering with feature scaling\n",
    "\n",
    "The figure below shows an image of the clustering obtained with feature scaling. What is your best guess as to what transformation was used to scale the features? If you have a guess you want to compare, manipulate `finance_features`,  run a K-means algorithm to create the new clusters, then plot the clusters using the Draw function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"kmclusters.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
